{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM9C2agmIQP2imMVSxqLD+I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1d5ef704c05a45c3bf42a34a663a3bcc":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_0a2ff275eb2e4215be60293de4535a92","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[32mâ §\u001b[0m Waiting for authorization\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">â §</span> Waiting for authorization\n</pre>\n"},"metadata":{}}]}},"0a2ff275eb2e4215be60293de4535a92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"RUrigf-v3dt2","executionInfo":{"status":"ok","timestamp":1763318359648,"user_tz":-360,"elapsed":34773,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e4b35df5-a833-41d5-bdcb-849be9b20924"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m467.8/467.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m261.3/261.3 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m438.8/438.8 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.8/76.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.1/74.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m375.5/375.5 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.0/87.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m127.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m753.9/753.9 kB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m788.2/788.2 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install dvc mlflow dagshub kaggle -q"]},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n","DAGSHUB_TOKEN = userdata.get('DAGSHUB_TOKEN')\n","GIT_USER_NAME = userdata.get('GIT_USER_NAME')\n","GIT_USER_EMAIL = userdata.get('GIT_USER_EMAIL')\n","KAGGLE_USERNAME = userdata.get('KAGGLE_USERNAME')\n","KAGGLE_KEY = userdata.get('KAGGLE_KEY')\n"],"metadata":{"id":"ZRoE58HT8wDb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["repo_owner=GIT_USER_NAME\n","repo_name=\"Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-Dagshub-Experiment\""],"metadata":{"id":"EVPlnsl688t7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git config --global user.name \"{GIT_USER_NAME}\"\n","!git config --global user.email \"{GIT_USER_EMAIL}\""],"metadata":{"id":"j30T9Jyr9m-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ['KAGGLE_USERNAME'] = KAGGLE_USERNAME\n","os.environ['KAGGLE_KEY'] = KAGGLE_KEY"],"metadata":{"id":"qZI2JMO29so3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://{GITHUB_TOKEN}@github.com/{repo_owner}/{repo_name}.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQ8VO9O9-Alb","executionInfo":{"status":"ok","timestamp":1763318370110,"user_tz":-360,"elapsed":685,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"ace2314f-3cb8-44c5-8291-518f9523a529"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-Dagshub-Experiment'...\n","remote: Enumerating objects: 15, done.\u001b[K\n","remote: Counting objects: 100% (15/15), done.\u001b[K\n","remote: Compressing objects: 100% (11/11), done.\u001b[K\n","remote: Total 15 (delta 1), reused 11 (delta 1), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (15/15), done.\n","Resolving deltas: 100% (1/1), done.\n"]}]},{"cell_type":"code","source":["%cd {repo_name}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8HwYJTmV-LHZ","executionInfo":{"status":"ok","timestamp":1763318370137,"user_tz":-360,"elapsed":26,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"650f7420-1ffb-43ca-9de3-3bc442b25b83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-Dagshub-Experiment\n"]}]},{"cell_type":"code","source":["import dagshub\n","dagshub.init(repo_owner=repo_owner, repo_name=repo_name, mlflow=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237,"referenced_widgets":["1d5ef704c05a45c3bf42a34a663a3bcc","0a2ff275eb2e4215be60293de4535a92"]},"id":"D_ZV7P0F-aw4","executionInfo":{"status":"ok","timestamp":1763318382707,"user_tz":-360,"elapsed":12568,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"b7541b22-3acf-4b10-8004-3c7483cad9bc"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["                                       \u001b[1mâ—â—â— AUTHORIZATION REQUIRED â—â—â—\u001b[0m                                        \n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">â—â—â— AUTHORIZATION REQUIRED â—â—â—</span>                                        \n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d5ef704c05a45c3bf42a34a663a3bcc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n","Open the following link in your browser to authorize the client:\n","https://dagshub.com/login/oauth/authorize?state=7e73b5f5-641a-4951-bb19-a0f2e1db9e37&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=807d096a88e4e738d5b9fdd03b5bcc54f6d1954b7ffa0ca4b3177913590ca326\n","\n","\n"]},{"output_type":"display_data","data":{"text/plain":[],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Accessing as nafizahamed8\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as nafizahamed8\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Initialized MLflow to track repo \n","\u001b[32m\"nafizahamed8/Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-Dagshub-Experiment\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo \n","<span style=\"color: #008000; text-decoration-color: #008000\">\"nafizahamed8/Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-Dagshub-Experiment\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Repository nafizahamed8/Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-Dagshub-Experiment \n","initialized!\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository nafizahamed8/Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-Dagshub-Experiment \n","initialized!\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqUQqDVbs0Eh","executionInfo":{"status":"ok","timestamp":1763318410837,"user_tz":-360,"elapsed":28125,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"038c757b-4cd7-4750-f26e-2158b1e43a44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!dvc init -f"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AP8PRYN1-6lY","executionInfo":{"status":"ok","timestamp":1763318412268,"user_tz":-360,"elapsed":1434,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"ac9e78a0-d9c5-4a9e-84fb-199fa4a3f9d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initialized DVC repository.\n","\n","You can now commit the changes to git.\n","\n","\u001b[31m+---------------------------------------------------------------------+\n","\u001b[0m\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n","\u001b[31m|\u001b[0m        DVC has enabled anonymous aggregate usage analytics.         \u001b[31m|\u001b[0m\n","\u001b[31m|\u001b[0m     Read the analytics documentation (and how to opt-out) here:     \u001b[31m|\u001b[0m\n","\u001b[31m|\u001b[0m             <\u001b[36mhttps://dvc.org/doc/user-guide/analytics\u001b[39m>              \u001b[31m|\u001b[0m\n","\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n","\u001b[31m+---------------------------------------------------------------------+\n","\u001b[0m\n","\u001b[33mWhat's next?\u001b[39m\n","\u001b[33m------------\u001b[39m\n","- Check out the documentation: <\u001b[36mhttps://dvc.org/doc\u001b[39m>\n","- Get help and share ideas: <\u001b[36mhttps://dvc.org/chat\u001b[39m>\n","- Star us on GitHub: <\u001b[36mhttps://github.com/iterative/dvc\u001b[39m>\n","\u001b[0m"]}]},{"cell_type":"code","source":["!mkdir -p \"/content/drive/My Drive/dvc_pneumonia_storage\"\n","#DVC to use the Google Drive folder as its storage\n","!dvc remote add -d gdrive_remote \"/content/drive/My Drive/dvc_pneumonia_storage\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YtrbNp5utRbr","executionInfo":{"status":"ok","timestamp":1763318413997,"user_tz":-360,"elapsed":1724,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"f1adae37-4391-4d7c-ac40-000faa1e06c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Setting 'gdrive_remote' as a default remote.\n","\u001b[0m"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Uu_XvYwTtg_F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile .gitignore\n","\n","# Ignore Python's cache files\n","__pycache__/\n","*.pyc\n","\n","# Ignore OS-specific files\n",".DS_Store\n","\n","# Ignore all environment files (which contain secrets)\n","*.env\n",".env\n","\n","# Ignore sensitive config files\n","config.ini\n","secrets.json\n","*.json\n","\n","# Ignore temporary data files\n","*.csv\n","*.pkl\n","\n","# Ignore the Colab-specific folder\n",".config/\n","\n","print(\"âœ… .gitignore file created.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VTfldsczAEjE","executionInfo":{"status":"ok","timestamp":1763318414029,"user_tz":-360,"elapsed":15,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"006ad1a2-f457-402a-97c1-c1d614145c1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting .gitignore\n"]}]},{"cell_type":"code","source":["!git add .gitignore\n","!git commit -m\" Adding.gitignore file for security\"\n","!git push origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3fxB7ZBbeiXh","executionInfo":{"status":"ok","timestamp":1763318414708,"user_tz":-360,"elapsed":676,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"454397bc-3657-4705-859d-51ba75c1349e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git restore <file>...\" to discard changes in working directory)\n","\t\u001b[31mmodified:   .dvc/config\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n","Everything up-to-date\n"]}]},{"cell_type":"code","source":["# Use the Kaggle API to download the dataset\n","!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vL-rCI2vnZ6m","executionInfo":{"status":"ok","timestamp":1763318501872,"user_tz":-360,"elapsed":87162,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"c609ad99-ae04-45f5-ce07-50698e1233a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset URL: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n","License(s): other\n","Downloading chest-xray-pneumonia.zip to /content/Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-Dagshub-Experiment\n"," 97% 2.22G/2.29G [00:24<00:02, 35.5MB/s]\n","100% 2.29G/2.29G [00:24<00:00, 99.1MB/s]\n"]}]},{"cell_type":"code","source":["# Unzip the data quietly\n","!unzip -q chest-xray-pneumonia.zip"],"metadata":{"id":"ohWZ9rVYniLM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir -p data/raw"],"metadata":{"id":"W-p_u6VUoi6g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mv chest_xray data/raw/"],"metadata":{"id":"-d6gXMPQovzF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm chest-xray-pneumonia.zip"],"metadata":{"id":"DRCwfieBpBrV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#dvc to track the data/raw folder\n","#creates a data/raw.dvc file\n","!dvc add data/raw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"vBfOTZG0pPpT","executionInfo":{"status":"ok","timestamp":1763318552237,"user_tz":-360,"elapsed":16841,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"0bf80531-f1b2-4e4a-db65-4eda0c4510fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l\u001b[32mâ ‹\u001b[0m Checking graph\n","Adding...:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\n","!\u001b[A\n","          |0.00 [00:00,     ?file/s]\u001b[A\n","  1% 54.0/3.88k [00:00<00:07, 539file/s{'info': ''}]\u001b[A\n"," 11% 444/3.88k [00:00<00:01, 2.51kfile/s{'info': ''}]\u001b[A\n"," 18% 696/3.88k [00:00<00:01, 1.63kfile/s{'info': ''}]\u001b[A\n"," 23% 886/3.88k [00:00<00:01, 1.66kfile/s{'info': ''}]\u001b[A\n"," 28% 1.07k/3.88k [00:00<00:02, 1.21kfile/s{'info': ''}]\u001b[A\n"," 31% 1.21k/3.88k [00:00<00:02, 1.08kfile/s{'info': ''}]\u001b[A\n"," 35% 1.34k/3.88k [00:01<00:02, 1.08kfile/s{'info': ''}]\u001b[A\n"," 38% 1.46k/3.88k [00:01<00:02, 1.08kfile/s{'info': ''}]\u001b[A\n"," 41% 1.57k/3.88k [00:01<00:02, 1.06kfile/s{'info': ''}]\u001b[A\n"," 44% 1.72k/3.88k [00:01<00:01, 1.15kfile/s{'info': ''}]\u001b[A\n"," 47% 1.83k/3.88k [00:01<00:01, 1.06kfile/s{'info': ''}]\u001b[A\n"," 51% 1.97k/3.88k [00:01<00:01, 1.13kfile/s{'info': ''}]\u001b[A\n"," 55% 2.13k/3.88k [00:01<00:01, 1.27kfile/s{'info': ''}]\u001b[A\n"," 59% 2.27k/3.88k [00:01<00:01, 1.26kfile/s{'info': ''}]\u001b[A\n"," 62% 2.41k/3.88k [00:01<00:01, 1.31kfile/s{'info': ''}]\u001b[A\n"," 66% 2.54k/3.88k [00:02<00:01, 1.27kfile/s{'info': ''}]\u001b[A\n"," 69% 2.67k/3.88k [00:02<00:00, 1.23kfile/s{'info': ''}]\u001b[A\n"," 72% 2.80k/3.88k [00:02<00:01, 1.03kfile/s{'info': ''}]\u001b[A\n"," 75% 2.91k/3.88k [00:02<00:00, 1.04kfile/s{'info': ''}]\u001b[A\n"," 78% 3.02k/3.88k [00:02<00:00, 1.07kfile/s{'info': ''}]\u001b[A\n"," 84% 3.26k/3.88k [00:02<00:00, 1.40kfile/s{'info': ''}]\u001b[A\n"," 88% 3.41k/3.88k [00:02<00:00, 1.29kfile/s{'info': ''}]\u001b[A\n"," 91% 3.54k/3.88k [00:02<00:00, 1.30kfile/s{'info': ''}]\u001b[A\n"," 96% 3.73k/3.88k [00:02<00:00, 1.46kfile/s{'info': ''}]\u001b[A\n"," 74% 3.88k/5.22k [00:03<00:01, 934file/s{'info': ''}]  \u001b[A\n"," 77% 4.00k/5.22k [00:03<00:01, 864file/s{'info': ''}]\u001b[A\n"," 79% 4.10k/5.22k [00:03<00:01, 841file/s{'info': ''}]\u001b[A\n"," 81% 4.20k/5.22k [00:03<00:01, 835file/s{'info': ''}]\u001b[A\n"," 82% 4.29k/5.22k [00:03<00:01, 826file/s{'info': ''}]\u001b[A\n"," 84% 4.38k/5.22k [00:03<00:01, 803file/s{'info': ''}]\u001b[A\n"," 86% 4.47k/5.22k [00:04<00:00, 802file/s{'info': ''}]\u001b[A\n"," 87% 4.55k/5.22k [00:04<00:00, 817file/s{'info': ''}]\u001b[A\n"," 89% 4.64k/5.22k [00:04<00:00, 836file/s{'info': ''}]\u001b[A\n"," 91% 4.73k/5.22k [00:04<00:00, 857file/s{'info': ''}]\u001b[A\n"," 92% 4.82k/5.22k [00:04<00:00, 864file/s{'info': ''}]\u001b[A\n"," 94% 4.91k/5.22k [00:04<00:00, 865file/s{'info': ''}]\u001b[A\n"," 96% 5.00k/5.22k [00:04<00:00, 883file/s{'info': ''}]\u001b[A\n"," 98% 5.09k/5.22k [00:04<00:00, 877file/s{'info': ''}]\u001b[A\n"," 99% 5.18k/5.22k [00:04<00:00, 713file/s{'info': ''}]\u001b[A\n"," 58% 5.26k/9.11k [00:05<00:06, 602file/s{'info': ''}]\u001b[A\n"," 63% 5.77k/9.11k [00:05<00:02, 1.64kfile/s{'info': ''}]\u001b[A\n"," 69% 6.29k/9.11k [00:05<00:01, 2.52kfile/s{'info': ''}]\u001b[A\n"," 74% 6.76k/9.11k [00:05<00:00, 3.10kfile/s{'info': ''}]\u001b[A\n"," 80% 7.25k/9.11k [00:05<00:00, 3.58kfile/s{'info': ''}]\u001b[A\n"," 85% 7.76k/9.11k [00:05<00:00, 4.00kfile/s{'info': ''}]\u001b[A\n"," 90% 8.24k/9.11k [00:05<00:00, 4.23kfile/s{'info': ''}]\u001b[A\n"," 96% 8.72k/9.11k [00:05<00:00, 4.38kfile/s{'info': ''}]\u001b[A\n"," 88% 9.17k/10.5k [00:06<00:00, 3.02kfile/s{'info': ''}]\u001b[A\n"," 91% 9.54k/10.5k [00:06<00:00, 1.87kfile/s{'info': ''}]\u001b[A\n"," 94% 9.82k/10.5k [00:06<00:00, 1.49kfile/s{'info': ''}]\u001b[A\n"," 96% 10.0k/10.5k [00:07<00:00, 1.31kfile/s{'info': ''}]\u001b[A\n"," 98% 10.2k/10.5k [00:07<00:00, 1.21kfile/s{'info': ''}]\u001b[A\n"," 99% 10.4k/10.5k [00:07<00:00, 1.08kfile/s{'info': ''}]\u001b[A\n"," 97% 10.5k/10.9k [00:07<00:00, 942file/s{'info': ''}]  \u001b[A\n"," 98% 10.9k/11.1k [00:07<00:00, 1.43kfile/s{'info': ''}]\u001b[A\n"," 74% 11.1k/15.0k [00:07<00:02, 1.34kfile/s{'info': ''}]\u001b[A\n"," 91% 13.7k/15.0k [00:08<00:00, 5.86kfile/s{'info': ''}]\u001b[A\n"," 92% 15.0k/16.3k [00:08<00:00, 7.01kfile/s{'info': ''}]\u001b[A\n","100% 16.3k/16.3k [00:08<00:00, 8.39kfile/s{'info': ''}]\u001b[A\n"," 99% 17.4k/17.6k [00:08<00:00, 8.11kfile/s{'info': ''}]\u001b[A\n","                                                       \u001b[A\n","!\u001b[A\n","  0% |          |0/? [00:00<?,    ?files/s]\u001b[A\n","                                           \u001b[A\n","Adding data/raw to cache:   0% 0.00/7.36k [00:00<?, ?file/s]\u001b[A\n","Adding data/raw to cache:   0% 0.00/7.36k [00:00<?, ?file/s{'info': ''}]\u001b[A\n","Adding data/raw to cache:   4% 258/7.36k [00:00<00:02, 2.58kfile/s{'info': ''}]\u001b[A\n","Adding data/raw to cache:   8% 574/7.36k [00:00<00:02, 2.92kfile/s{'info': ''}]\u001b[A\n","Adding data/raw to cache:  12% 866/7.36k [00:00<00:02, 2.38kfile/s{'info': ''}]\u001b[A\n"," 15% 1.12k/7.36k [00:00<00:02, 2.43kfile/s{'info': ''}]                        \u001b[A\n"," 19% 1.37k/7.36k [00:00<00:02, 2.31kfile/s{'info': ''}]\u001b[A\n"," 22% 1.60k/7.36k [00:00<00:03, 1.54kfile/s{'info': ''}]\u001b[A\n"," 25% 1.84k/7.36k [00:00<00:03, 1.73kfile/s{'info': ''}]\u001b[A\n"," 29% 2.14k/7.36k [00:01<00:02, 1.98kfile/s{'info': ''}]\u001b[A\n"," 32% 2.36k/7.36k [00:01<00:03, 1.32kfile/s{'info': ''}]\u001b[A\n"," 34% 2.54k/7.36k [00:01<00:03, 1.31kfile/s{'info': ''}]\u001b[A\n"," 37% 2.72k/7.36k [00:01<00:03, 1.42kfile/s{'info': ''}]\u001b[A\n"," 39% 2.89k/7.36k [00:01<00:03, 1.25kfile/s{'info': ''}]\u001b[A\n"," 41% 3.03k/7.36k [00:01<00:03, 1.28kfile/s{'info': ''}]\u001b[A\n"," 45% 3.29k/7.36k [00:01<00:02, 1.58kfile/s{'info': ''}]\u001b[A\n"," 48% 3.56k/7.36k [00:02<00:02, 1.84kfile/s{'info': ''}]\u001b[A\n"," 52% 3.85k/7.36k [00:02<00:01, 2.12kfile/s{'info': ''}]\u001b[A\n"," 56% 4.15k/7.36k [00:02<00:01, 2.36kfile/s{'info': ''}]\u001b[A\n"," 60% 4.40k/7.36k [00:02<00:02, 1.42kfile/s{'info': ''}]\u001b[A\n"," 63% 4.60k/7.36k [00:02<00:02, 1.31kfile/s{'info': ''}]\u001b[A\n"," 65% 4.80k/7.36k [00:02<00:01, 1.44kfile/s{'info': ''}]\u001b[A\n"," 68% 5.04k/7.36k [00:03<00:01, 1.64kfile/s{'info': ''}]\u001b[A\n"," 71% 5.23k/7.36k [00:03<00:01, 1.42kfile/s{'info': ''}]\u001b[A\n"," 73% 5.40k/7.36k [00:03<00:01, 1.39kfile/s{'info': ''}]\u001b[A\n"," 76% 5.56k/7.36k [00:03<00:01, 1.42kfile/s{'info': ''}]\u001b[A\n"," 78% 5.72k/7.36k [00:03<00:01, 1.46kfile/s{'info': ''}]\u001b[A\n"," 80% 5.88k/7.36k [00:03<00:01, 1.44kfile/s{'info': ''}]\u001b[A\n"," 83% 6.08k/7.36k [00:03<00:00, 1.56kfile/s{'info': ''}]\u001b[A\n"," 85% 6.24k/7.36k [00:03<00:00, 1.52kfile/s{'info': ''}]\u001b[A\n"," 87% 6.40k/7.36k [00:04<00:00, 1.43kfile/s{'info': ''}]\u001b[A\n"," 90% 6.62k/7.36k [00:04<00:00, 1.61kfile/s{'info': ''}]\u001b[A\n"," 92% 6.79k/7.36k [00:04<00:00, 1.39kfile/s{'info': ''}]\u001b[A\n"," 94% 6.93k/7.36k [00:04<00:00, 1.16kfile/s{'info': ''}]\u001b[A\n"," 96% 7.08k/7.36k [00:04<00:00, 1.24kfile/s{'info': ''}]\u001b[A\n","100% 7.33k/7.36k [00:04<00:00, 1.53kfile/s{'info': ''}]\u001b[A\n","                                                       \u001b[A\n","!\u001b[A\n","          |0.00 [00:00,    ?files/s]\u001b[A\n","Adding...: 100% 1/1 [00:14<00:00, 14.83s/file{'info': ''}]\n","\n","To track the changes with git, run:\n","\n","\tgit add data/raw.dvc data/.gitignore\n","\n","To enable auto staging, run:\n","\n","\tdvc config core.autostage true\n","\u001b[0m"]}]},{"cell_type":"code","source":["!dvc push"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"CUBJukUOuY_O","executionInfo":{"status":"ok","timestamp":1763318559423,"user_tz":-360,"elapsed":7183,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"c92ec05e-782d-4692-b808-3cf9816f37f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting          |17.6k [00:02, 8.22kentry/s]\n","Pushing\n","Querying remote cache:   0% 0/1 [00:00<?, ?files/s]\u001b[A\n","Querying remote cache:   0% 0/1 [00:00<?, ?files/s{'info': ''}]\u001b[A\n","Querying remote cache: 100% 1/1 [00:02<00:00,  2.42s/files{'info': ''}]\u001b[A\n","Pushing\n","Everything is up to date.\n","\u001b[0m"]}]},{"cell_type":"code","source":["!git add data/raw.dvc"],"metadata":{"id":"YglAf_P5vRJj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git commit -m\"Add raw pneumonia dataset (v1)\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0jjIN1UbvYKd","executionInfo":{"status":"ok","timestamp":1763318559661,"user_tz":-360,"elapsed":124,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"bd695e94-5402-4bf7-a46e-ab7c3792b8a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git restore <file>...\" to discard changes in working directory)\n","\t\u001b[31mmodified:   .dvc/config\u001b[m\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\t\u001b[31mdata/.gitignore\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}]},{"cell_type":"code","source":["!git push origin main"],"metadata":{"id":"gHFFW9CJvgHL","executionInfo":{"status":"ok","timestamp":1763318560156,"user_tz":-360,"elapsed":494,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"7b609185-64aa-4358-9517-da825cb0a0f5","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Everything up-to-date\n"]}]},{"cell_type":"code","source":["%%writefile preprocess.py\n","\n","import os\n","import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","def get_data_loaders(data_dir, batch_size, img_size):\n","    \"\"\"Applies transforms and creates DataLoaders.\"\"\"\n","\n","    # Define a dictionary of transforms\n","    data_transforms = {\n","        # Train transform: includes data augmentation\n","        'train': transforms.Compose([\n","            transforms.Resize((img_size, img_size)),\n","            transforms.Grayscale(num_output_channels=3), # Convert grayscale to 3-channel\n","            transforms.RandomHorizontalFlip(),           # Augmentation\n","            transforms.RandomRotation(10),               # Augmentation\n","            transforms.ToTensor(),                       # Convert to tensor and scale [0, 1]\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet stats\n","        ]),\n","        # Validation transform: no augmentation\n","        'val': transforms.Compose([\n","            transforms.Resize((img_size, img_size)),\n","            transforms.Grayscale(num_output_channels=3),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ]),\n","    }\n","\n","    # Create PyTorch ImageFolder Datasets\n","    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n","                      for x in ['train', 'val']}\n","\n","    # Create PyTorch DataLoaders\n","    dataloaders = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=(x=='train'), num_workers=2, pin_memory=True)\n","                   for x in ['train', 'val']}\n","\n","    # Return the loaders and the class names (['NORMAL', 'PNEUMONIA'])\n","    return dataloaders, image_datasets['train'].classes\n","\n","def calculate_class_weights(data_dir):\n","    \"\"\"Calculates class weights to handle the data imbalance.\"\"\"\n","    # Count the number of files in each training class folder\n","    count_normal = len(os.listdir(os.path.join(data_dir, 'train', 'NORMAL')))\n","    count_pneumonia = len(os.listdir(os.path.join(data_dir, 'train', 'PNEUMONIA')))\n","    total = count_normal + count_pneumonia\n","\n","    # Calculate weights. The rare class (NORMAL) will get a high weight.\n","    weight_for_0 = (1 / count_normal) * (total / 2.0)\n","    weight_for_1 = (1 / count_pneumonia) * (total / 2.0)\n","\n","    class_weights = torch.tensor([weight_for_0, weight_for_1])\n","    print(f\"Class Weights: NORMAL(0): {weight_for_0:.2f}, PNEUMONIA(1): {weight_for_1:.2f}\")\n","    return class_weights\n","\n","print(\"\\nâœ… preprocess.py script created successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"neXyA-b0-Tr0","executionInfo":{"status":"ok","timestamp":1763319166052,"user_tz":-360,"elapsed":18,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"d87bb140-09da-4bc3-ed86-a51fd9699bf6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting preprocess.py\n"]}]},{"cell_type":"code","source":["%%writefile train.py\n","# This entire cell is saved as 'train.py'\n","\n","# 1. --- Imports ---\n","import os\n","import torch\n","import argparse\n","import mlflow\n","import dagshub\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader\n","\n","def get_model(model_name, num_classes=2):\n","    \"\"\"Loads a pretrained model and replaces the final layer.\"\"\"\n","    model = None\n","    if model_name == \"mobilenetv2\":\n","        model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n","        num_ftrs = model.classifier[1].in_features\n","        model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n","\n","    elif model_name == \"efficientnetb0\":\n","        model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n","        num_ftrs = model.classifier[1].in_features\n","        model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n","    else:\n","        raise Exception(\"Invalid model. Choose 'mobilenetv2' or 'efficientnetb0'\")\n","    return model\n","\n","def get_data_loaders(data_dir, batch_size, img_size):\n","    \"\"\"Applies transforms and creates DataLoaders.\"\"\"\n","\n","    data_transforms = {\n","        'train': transforms.Compose([\n","            transforms.Resize((img_size, img_size)),\n","            transforms.Grayscale(num_output_channels=3), # Convert grayscale to 3-channel\n","            transforms.RandomHorizontalFlip(),           # Augmentation\n","            transforms.RandomRotation(10),               # Augmentation\n","            transforms.ToTensor(),                       # Convert to tensor and scale [0, 1]\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet stats\n","        ]),\n","        'val': transforms.Compose([\n","            transforms.Resize((img_size, img_size)),\n","            transforms.Grayscale(num_output_channels=3),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ]),\n","    }\n","\n","    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n","                      for x in ['train', 'val']}\n","\n","    dataloaders = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=(x=='train'), num_workers=2, pin_memory=True)\n","                   for x in ['train', 'val']}\n","\n","    return dataloaders, image_datasets['train'].classes\n","\n","def calculate_class_weights(data_dir):\n","    \"\"\"Calculates class weights to handle the data imbalance.\"\"\"\n","    count_normal = len(os.listdir(os.path.join(data_dir, 'train', 'NORMAL')))\n","    count_pneumonia = len(os.listdir(os.path.join(data_dir, 'train', 'PNEUMONIA')))\n","    total = count_normal + count_pneumonia\n","\n","    weight_for_0 = (1 / count_normal) * (total / 2.0)\n","    weight_for_1 = (1 / count_pneumonia) * (total / 2.0)\n","\n","    class_weights = torch.tensor([weight_for_0, weight_for_1])\n","    print(f\"Class Weights: NORMAL(0): {weight_for_0:.2f}, PNEUMONIA(1): {weight_for_1:.2f}\")\n","    return class_weights\n","\n","# --- 3. Main Training Function ---\n","\n","def main(args):\n","    \"\"\"The main function that runs the entire pipeline.\"\"\"\n","\n","\n","    dagshub.init(repo_owner=args.repo_owner, repo_name=args.repo_name, mlflow=True)\n","    print(\"DagsHub MLflow tracking initialized.\")\n","\n","    data_dir = \"data/raw/chest_xray\"\n","    dataloaders, class_names = get_data_loaders(data_dir, args.batch_size, args.img_size)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Using device: {device}\")\n","    model = get_model(args.model_name).to(device)\n","\n","    class_weights = calculate_class_weights(data_dir).to(device)\n","    criterion = nn.CrossEntropyLoss(weight=class_weights)\n","\n","    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","\n","    with mlflow.start_run():\n","        print(f\"Starting MLflow run for model: {args.model_name}\")\n","\n","        mlflow.log_params(vars(args))\n","        mlflow.log_param(\"class_names\", class_names)\n","        mlflow.log_param(\"device\", device)\n","\n","        # --- Training Loop --- (unchanged)\n","        for epoch in range(args.epochs):\n","            for phase in ['train', 'val']:\n","                if phase == 'train':\n","                    model.train()\n","                else:\n","                    model.eval()\n","\n","                running_loss = 0.0\n","                running_corrects = 0\n","\n","                for inputs, labels in dataloaders[phase]:\n","                    inputs, labels = inputs.to(device), labels.to(device)\n","                    optimizer.zero_grad()\n","\n","                    with torch.set_grad_enabled(phase == 'train'):\n","                        outputs = model(inputs)\n","                        _, preds = torch.max(outputs, 1)\n","                        loss = criterion(outputs, labels)\n","\n","                        if phase == 'train':\n","                            loss.backward()\n","                            optimizer.step()\n","\n","                    running_loss += loss.item() * inputs.size(0)\n","                    running_corrects += torch.sum(preds == labels.data)\n","\n","                epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","                epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","                print(f\"Epoch {epoch+1}/{args.epochs} | {phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n","\n","                mlflow.log_metric(f\"{phase}_loss\", epoch_loss, step=epoch)\n","                mlflow.log_metric(f\"{phase}_acc\", epoch_acc, step=epoch)\n","\n","\n","        print(\"Training complete. Logging model artifact...\")\n","\n","        model_path = f\"{args.model_name}.pth\"\n","        torch.save(model.state_dict(), model_path)\n","        mlflow.log_artifact(model_path)\n","\n","        print(f\"Model saved to {model_path} and logged to MLflow.\")\n","\n","# --- 4. Argument Parsing (Script Entry Point) ---\n","if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser(description=\"Pneumonia Classification Training Pipeline\")\n","\n","    parser.add_argument(\"--repo_owner\", type=str, default=os.getenv(\"DAGSHUB_REPO_OWNER\"), help=\"DagsHub repo owner (your username)\")\n","    parser.add_argument(\"--repo_name\", type=str, default=os.getenv(\"DAGSHUB_REPO_NAME\"), help=\"DagsHub repo name\")\n","\n","    parser.add_argument(\"--model_name\", type=str, default=\"mobilenetv2\", help=\"Model to train (mobilenetv2 or efficientnetb0)\")\n","    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"Learning rate\")\n","    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size\")\n","    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of epochs\")\n","    parser.add_argument(\"--img_size\", type=int, default=224, help=\"Image size\")\n","\n","    args = parser.parse_args()\n","\n","\n","    if not args.repo_owner or not args.repo_name:\n","        raise ValueError(\"Error: DAGSHUB_REPO_OWNER and DAGSHUB_REPO_NAME must be set as environment variables.\")\n","\n","    main(args)\n","\n","print(\"\\nâœ… train.py script updated successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-NZ2usQ4Bqg-","executionInfo":{"status":"ok","timestamp":1763319477338,"user_tz":-360,"elapsed":27,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"113f4109-b3e8-42a4-a138-0c5e4d15e9ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting train.py\n"]}]},{"cell_type":"code","source":["!dvc pull"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UIOlcE89__k5","executionInfo":{"status":"ok","timestamp":1763319488120,"user_tz":-360,"elapsed":2217,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"414e27f7-815f-4154-901e-d8be0c9458d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting          |0.00 [00:00,    ?entry/s]\n","Fetching\n","!\u001b[A\n","  0% |          |0/? [00:00<?,    ?files/s]\u001b[A\n","Fetching\n","Building workspace index          |17.6k [00:00, 23.7kentry/s]\n","Comparing indexes          |17.6k [00:00, 42.0kentry/s]\n","Applying changes          |0.00 [00:00,     ?file/s]\n","Everything is up to date.\n","\u001b[0m"]}]},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","\n","print(\"--- 1. Pulling DVC data ---\")\n","!dvc pull\n","\n","os.environ[\"DAGSHUB_REPO_OWNER\"] = userdata.get('GIT_USER_NAME')\n","os.environ[\"DAGSHUB_REPO_NAME\"] = \"Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-Dagshub-Experiment\"\n","\n","# -----------------------------------------------------------------\n","print(\"\\n--- 3. Starting Experiment 1: MobileNetV2 ---\")\n","# -----------------------------------------------------------------\n","\n","!python train.py \\\n","    --model_name mobilenetv2 \\\n","    --lr 0.001 \\\n","    --epochs 5 \\\n","    --batch_size 32\n","\n","# -----------------------------------------------------------------\n","print(\"\\n--- 4. Starting Experiment 2: EfficientNetB0 ---\")\n","# -----------------------------------------------------------------\n","!python train.py \\\n","    --model_name efficientnetb0 \\\n","    --lr 0.001 \\\n","    --epochs 5 \\\n","    --batch_size 32\n","\n","print(\"\\nâœ… Both experiments are complete!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4RKaOlzzBiQv","executionInfo":{"status":"ok","timestamp":1763320451613,"user_tz":-360,"elapsed":959327,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"f4151fec-c02f-4a29-d1cc-cd700e55cfc1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- 1. Pulling DVC data ---\n","Collecting          |0.00 [00:00,    ?entry/s]\n","Fetching\n","!\u001b[A\n","  0% |          |0/? [00:00<?,    ?files/s]\u001b[A\n"," 99% 7284/7356 [00:00<00:00, 72838.68files/s{'info': ''}]\u001b[A\n","Fetching\n","Building workspace index          |17.6k [00:01, 14.4kentry/s]\n","Comparing indexes          |17.6k [00:00, 38.2kentry/s]\n","Applying changes          |0.00 [00:00,     ?file/s]\n","Everything is up to date.\n","\u001b[0m\n","--- 3. Starting Experiment 1: MobileNetV2 ---\n","Accessing as nafizahamed8\n","Initialized MLflow to track repo \n","\u001b[32m\"nafizahamed8/Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-D\u001b[0m\n","\u001b[32magshub-Experiment\"\u001b[0m\n","Repository \n","nafizahamed8/Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-Da\n","gshub-Experiment initialized!\n","DagsHub MLflow tracking initialized.\n","Using device: cuda\n","Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n","100% 13.6M/13.6M [00:00<00:00, 124MB/s]\n","Class Weights: NORMAL(0): 1.94, PNEUMONIA(1): 0.67\n","Starting MLflow run for model: mobilenetv2\n","Epoch 1/5 | train Loss: 0.1481 Acc: 0.9465\n","Epoch 1/5 | val Loss: 1.1308 Acc: 0.6875\n","Epoch 2/5 | train Loss: 0.0960 Acc: 0.9636\n","Epoch 2/5 | val Loss: 0.1500 Acc: 0.9375\n","Epoch 3/5 | train Loss: 0.0745 Acc: 0.9699\n","Epoch 3/5 | val Loss: 0.2285 Acc: 0.9375\n","Epoch 4/5 | train Loss: 0.0677 Acc: 0.9741\n","Epoch 4/5 | val Loss: 0.2801 Acc: 0.8750\n","Epoch 5/5 | train Loss: 0.0603 Acc: 0.9758\n","Epoch 5/5 | val Loss: 0.3725 Acc: 0.7500\n","Training complete. Logging model artifact...\n","Model saved to mobilenetv2.pth and logged to MLflow.\n","ğŸƒ View run bemused-grouse-485 at: https://dagshub.com/nafizahamed8/Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-Dagshub-Experiment.mlflow/#/experiments/0/runs/edc6b58201ac48f5a153b14f56a61dd2\n","ğŸ§ª View experiment at: https://dagshub.com/nafizahamed8/Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-Dagshub-Experiment.mlflow/#/experiments/0\n","\n","âœ… train.py script updated successfully.\n","\n","--- 4. Starting Experiment 2: EfficientNetB0 ---\n","Accessing as nafizahamed8\n","Initialized MLflow to track repo \n","\u001b[32m\"nafizahamed8/Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-D\u001b[0m\n","\u001b[32magshub-Experiment\"\u001b[0m\n","Repository \n","nafizahamed8/Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-Da\n","gshub-Experiment initialized!\n","DagsHub MLflow tracking initialized.\n","Using device: cuda\n","Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n","100% 20.5M/20.5M [00:00<00:00, 175MB/s]\n","Class Weights: NORMAL(0): 1.94, PNEUMONIA(1): 0.67\n","Starting MLflow run for model: efficientnetb0\n","Epoch 1/5 | train Loss: 0.1597 Acc: 0.9373\n","Epoch 1/5 | val Loss: 0.1240 Acc: 0.9375\n","Epoch 2/5 | train Loss: 0.0813 Acc: 0.9678\n","Epoch 2/5 | val Loss: 0.2065 Acc: 0.9375\n","Epoch 3/5 | train Loss: 0.0689 Acc: 0.9741\n","Epoch 3/5 | val Loss: 0.0976 Acc: 1.0000\n","Epoch 4/5 | train Loss: 0.0576 Acc: 0.9770\n","Epoch 4/5 | val Loss: 0.0304 Acc: 1.0000\n","Epoch 5/5 | train Loss: 0.0405 Acc: 0.9845\n","Epoch 5/5 | val Loss: 0.0949 Acc: 1.0000\n","Training complete. Logging model artifact...\n","Model saved to efficientnetb0.pth and logged to MLflow.\n","ğŸƒ View run mysterious-perch-708 at: https://dagshub.com/nafizahamed8/Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-Dagshub-Experiment.mlflow/#/experiments/0/runs/d3ceeb57c81349fe8b25f062e55dde83\n","ğŸ§ª View experiment at: https://dagshub.com/nafizahamed8/Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-Dagshub-Experiment.mlflow/#/experiments/0\n","\n","âœ… train.py script updated successfully.\n","\n","âœ… Both experiments are complete!\n"]}]},{"cell_type":"code","source":["# %%writefile preprocess.py\n","# import os\n","# import torch\n","# from torchvision import datasets, transforms\n","# from torch.utils.data import DataLoader\n","\n","# def get_loader(data_dir,batch_size,img_size):\n","#   \"\"\" Applies transform and creates DataLoader\"\"\"\n","#   # PyTorch utility that chains multiple preprocessing steps together.\n","#   data_transform={\n","#       'train':transforms.Compose([\n","#           transforms.Resize((img_size,img_size)),\n","#           # Converts the 1-channel grayscale X-ray images into 3-channel (RGB) images\n","#           transforms.Grayscale(num_output_channels=3),\n","#           transforms.RandomHorizontalFlip(),\n","#           transforms.RandomRotation(10),\n","#           transforms.ToTensor(),\n","#           # Standardizes the pixel values using the specific mean and standard deviation from the ImageNet dataset. This matches the exact preprocessing the model originally used.\n","#           transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet stats\n","\n","#       ])\n","#       # Validation transform: no augmentation\n","#       'val': transforms.Compose([\n","#           transforms.Resize((img_size, img_size)),\n","#           transforms.Grayscale(num_output_channels=3),\n","#           transforms.ToTensor(),\n","#           transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","#         ]),\n","#   }\n","#   image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n","#                       for x in ['train', 'val']}\n","\n","#   dataloaders = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=(x=='train'), num_workers=2, pin_memory=True)\n","#                    for x in ['train', 'val']}\n","\n","#   return dataloaders, image_datasets['train'].classes\n","\n","#   def calculate_class_weights(data_dir):\n","#     \"\"\"Calculates class weights to handle the data imbalance.\"\"\"\n","#     count_normal=len(os.listdir(os.path.join(data_dir,'train','NORMAL')))\n","#     count_pneumonia=len(os.listdir(os.path.join(data_dir,'train','PNEUMONIA')))\n","#     total=count_normal+count_pneumonia\n","#     weight_for_0=(1/count_normal) *(total/2.0)\n","#     weight_for_1=(1/count_pneumonia) *(total/2.0)\n","#     return class_weights\n","\n","#   # accepts the args (arguments) from the command line.\n","#   def main(args):\n","#     \"\"\"The main function that runs the entire pipeline.\"\"\"\n","#     dagshub.init(repo_owner=args.repo_owner, repo_name=args.repo_name, mlflow=True)\n","#     data\n","\n","# data_dir=\"data/raw/chest_xray\"\n","# dataloaders, class_names = get_data_loaders(data_dir, args.batch_size, args.img_size)\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# print(f\"Using device: {device}\")\n","# model = get_model(args.model_name).to(device)\n","# class_weights=calculate_class_weights(data_dir).to(device)\n","# criterion = nn.CrossEntropyLoss(weight=class_weights)\n","# optimizer=optim.Adam(model.parameters(),lr=args.lr)\n","\n","# with mlflow.startrun():\n","#   print(f\"Starting MLflow run for model: {args.model_name}\")\n","#   mlflow.log_params(vars(args))\n","#   mlflow.log_param(\"data_dir\",data_dir)\n","#   mlflow.log_param(\"class_names\",class_names)\n","#   mlflow.log_param(\"device\",device)\n"],"metadata":{"id":"TpE-gszC9Wy9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for epoch in range(args.epochs):\n","#   for phase in ['train','val']:\n","#     if phase =='train':\n","#       model.train()\n","#     else:\n","#       model.eval()\n","# running_loss=0.0\n","# running_corrects=0\n","# for inputs,labels in dataloaders[phase]:\n","#   inputs=inputs.to(device)\n","#   labels=labels.to(device)\n","#   optimizer.zero_grad()\n","# with torch.set_grad_enabled(phase=='train'):\n","#   outputs=model(inputs)\n","#   _,preds=torch.max(outputs,1)\n","#   loss=criterion(outputs,labels)\n","\n","#   if phase=='train':\n","#     loss.backward()\n","#     optimizer.step()\n","#     running_loss+=loss.item()*inputs.size(0)\n","#     running_corrects+=torch.sum(preds==labels.data)\n","#     epoch_loss=running_loss/len(dataloaders[phase].dataset)\n","#     epoch_acc=running_corrects.double()/len(dataloaders[phase].dataset)\n","#     print(f\"Epoch {epoch+1}/{args.epochs} | {phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n","#     mlflow.log_metric(f\"{phase}_loss\", epoch_loss, step=epoch)\n","#     mlflow.log_metric(f\"{phase}_acc\", epoch_acc, step=epoch)\n","\n","# model_path=f\"{args.model_name}.pth\"\n","# torch.save(model.state_dict(),model_path)\n","# mlflow.log_artifact(model_path)"],"metadata":{"id":"IezxQaaXgqdv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# if __name__=\"__main__\":\n","#   parser=argparse.ArgumentParser(...)\n","#   parser.add_argument(\"--repo_owner\", ...)\n","#   parser.add_argument(\"--repo_name\", ...)\n","#   parser.add_argument(\"--model_name\", ...)\n","#   parser.add_argument(\"--lr\", ...)\n","#   parser.add_argument(\"--batch_size\", ...)\n","#   parser.add_argument(\"--epochs\", ...)\n","#   parser.add_argument(\"--img_size\", ...)\n","\n","# args = parser.parse_args()\n","\n","# os.environ[\"GIT_USER_NAME\"] = userdata.get('GIT_USER_NAME')\n","#  main(args)\n"],"metadata":{"id":"Uu0srC3Z6dyQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %%writefile train.py\n","# import os\n","# import torch\n","# import argparse #mlops tools lets parse arguement from command line\n","# import mlflow\n","# import dagshub\n","# import torch.nn as nn\n","# import torch.optim as optim\n","# from torchvision import datasets,transforms,models\n","# from torch.utils import DataLoader\n","# from google.colab import userdata\n","\n","# def get_model(model_name,num_classes=2):\n","#   \"\"\"Loads a pretrained model and replace the final layers\"\"\"\n","#   model=None\n","#   if model_name=\"mobilenetv2\":\n","#     model=models.mobilenet_v2(weights=models.MobileNet_v2_Weights.DEFAULT)\n","#     num_fltrs=model.classifier[1].in_features\n","#     model.classifier[1]=nn.Linear(num_fltrs,num_classes)\n","#   elif model_name == \"efficientnetb0\":\n","#         model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n","#         num_ftrs = model.classifier[1].in_features\n","#         model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n","#   else:\n","#     raise Exception(\"Invalid model. Choose 'mobilenetv2' or 'efficientnetb0'\")\n","#   return model"],"metadata":{"id":"9lS6K6hAvjgz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git add ."],"metadata":{"id":"IbGPgv-8Fw9h","executionInfo":{"status":"ok","timestamp":1763321394250,"user_tz":-360,"elapsed":108,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["!git add preprocess.py train.py"],"metadata":{"id":"E1pVMf_RG0UC","executionInfo":{"status":"ok","timestamp":1763321390704,"user_tz":-360,"elapsed":112,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["!git commit -m\"Add modular preprocess.py and train.py scripts\""],"metadata":{"id":"3XFhiOc4H-Lg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763321401217,"user_tz":-360,"elapsed":133,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"f89429df-22e1-4b7c-d591-18beadcc3d2e"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["[main 3f757b2] Add modular preprocess.py and train.py scripts\n"," 6 files changed, 225 insertions(+)\n"," create mode 100644 data/.gitignore\n"," create mode 100644 efficientnetb0.pth\n"," create mode 100644 mobilenetv2.pth\n"," create mode 100644 preprocess.py\n"," create mode 100644 train.py\n"]}]},{"cell_type":"code","source":["!git push origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2BmFr60CJVou","executionInfo":{"status":"ok","timestamp":1763321419854,"user_tz":-360,"elapsed":5235,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"30970870-57fc-4b90-cd75-7e53a8c2c2c2"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Enumerating objects: 14, done.\n","Counting objects:   7% (1/14)\rCounting objects:  14% (2/14)\rCounting objects:  21% (3/14)\rCounting objects:  28% (4/14)\rCounting objects:  35% (5/14)\rCounting objects:  42% (6/14)\rCounting objects:  50% (7/14)\rCounting objects:  57% (8/14)\rCounting objects:  64% (9/14)\rCounting objects:  71% (10/14)\rCounting objects:  78% (11/14)\rCounting objects:  85% (12/14)\rCounting objects:  92% (13/14)\rCounting objects: 100% (14/14)\rCounting objects: 100% (14/14), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (9/9), done.\n","Writing objects: 100% (10/10), 22.42 MiB | 7.08 MiB/s, done.\n","Total 10 (delta 0), reused 0 (delta 0), pack-reused 0\n","To https://github.com/nafizahamed8/Pneumonia-XRay-Classification--Mobilenetv2-EfficientnetBO-MLflow-Dagshub-Experiment.git\n","   5fbfe77..3f757b2  main -> main\n"]}]},{"cell_type":"code","source":["NOTEBOOK_NAME = \"mlops-mlflow-experiment.ipynb\"\n"],"metadata":{"id":"XGRqW5JDJZ_d","executionInfo":{"status":"ok","timestamp":1763321789484,"user_tz":-360,"elapsed":25,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["!ls -l \"/content/drive/My Drive/Colab Notebooks/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"J5U4IDPwKdSD","executionInfo":{"status":"ok","timestamp":1763321836109,"user_tz":-360,"elapsed":215,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"ebfda03b-b481-461c-9c0f-63314f35a335"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["total 18581\n","-rw------- 1 root root    3252 Dec 14  2024 '3d unet.ipynb'\n","-rw------- 1 root root 1600252 Jan 17  2025 '3d Unet training.ipynb'\n","-rw------- 1 root root  255065 Oct  3 17:39 'Arima vs lstm.ipynb'\n","-rw------- 1 root root  184153 Jan 13  2025 'Bert nlp.ipynb'\n","-rw------- 1 root root    7001 Dec  9  2024 'Biagram Language Model.ipynb'\n","-rw------- 1 root root  377806 Jan 13  2025 'BI DIRECTIONAL LSTM.ipynb'\n","-rw------- 1 root root    5134 Jul 23 07:41 'Building a Rag system in Langchain.ipynb'\n","-rw------- 1 root root   93719 Sep 22 18:30 'Building Large Language Model From Scratch_1.ipynb'\n","-rw------- 1 root root    5334 May  4  2025  code.ipynb\n","-rw------- 1 root root   18609 Mar  2  2025 'Copy of backpropagation-regression.ipynb'\n","-rw------- 1 root root   45611 Dec 10  2024 'cryptocurrency price prediction.ipynb'\n","-rw------- 1 root root    3125 Nov 28  2024 'CryptoPrice prediction.ipynb'\n","-rw------- 1 root root  233645 Feb 17  2025 'Data preprocessing of UCIHAR Dataset.ipynb'\n","-rw------- 1 root root  187346 Aug 20 19:48 'dataset reviewing.ipynb'\n","-rw------- 1 root root 1540050 Sep 14 13:40 'Deep Neural Network in PyTorch.ipynb'\n","-rw------- 1 root root  178118 Dec  1  2024 'email classifier.ipynb'\n","-rw------- 1 root root  128234 Jul 26 12:47 'FinalProjectcolab .ipynb'\n","-rw------- 1 root root  164284 Jul 26 12:55 'FIXED_FinalProjectcolab  (1).ipynb'\n","-rw------- 1 root root  128243 Jul 26 13:05 'FIXED_FinalProjectcolab .ipynb'\n","-rw------- 1 root root 2663565 Sep  1 18:30  FIXED_madewithml.ipynb\n","-rw------- 1 root root   54781 Sep  7 18:44  FIXED_Steps_to_build_a_neural_network_on_FashionMNIST.ipynb\n","-rw------- 1 root root   29589 Nov  2  2024 'gradient descent 2.ipynb'\n","-rw------- 1 root root  209092 Nov  1  2024 'gradient descent.ipynb'\n","-rw------- 1 root root    5626 Aug 23 18:48 'Implementing CNN from Scratch.ipynb'\n","-rw------- 1 root root  103870 Aug 27 17:27 'Implementing Deep CNN with PyTorch.ipynb'\n","-rw------- 1 root root   66974 Nov 16 19:37  mlops-mlflow-experiment.ipynb\n","-rw------- 1 root root   33021 Nov  5  2024 'multiple linear regression.ipynb'\n","-rw------- 1 root root  990708 Oct  4 12:23 'Nafiz Uddin Ahamed Stock Price Forecasting Analysis Using ARIMA and LSTM.ipynb'\n","-rw------- 1 root root    4261 Nov 27  2024  NaiveBayesClassifier.ipynb\n","-rw------- 1 root root   76203 Sep  5 14:19 'neural network in PyTorch.ipynb'\n","-rw------- 1 root root   91330 Aug 31 19:13 'Neural Network.ipynb'\n","-rw------- 1 root root  839085 Jan 14  2025 'nlp 1.ipynb'\n","-rw------- 1 root root  108527 Nov 23  2024 'perceptron trick.ipynb'\n","-rw------- 1 root root  192607 Nov 11  2024 'polynomial regression.ipynb'\n","-rw------- 1 root root  119339 Jul 24 19:10  projectDemo.ipynb\n","-rw------- 1 root root   70462 Nov  2 18:34 'spam detection.ipynb'\n","-rw------- 1 root root   81516 Dec 10  2024 'svm price prediction.ipynb'\n","-rw------- 1 root root  269613 Oct  3 14:55 'Time Series Forecasting Stock Prices Using ARIMA Model and LSTM.ipynb'\n","-rw------- 1 root root 1438730 Aug 23 14:30 'Training 3D Unet Architecture For Brain Tumor Segmentation.ipynb'\n","-rw------- 1 root root  140418 Feb 17  2025 'Transformer Based Approach UCI HAR Dataset.ipynb'\n","-rw------- 1 root root    6071 Dec 30  2024 'transformer encoder.ipynb'\n","-rw------- 1 root root   77097 Jan 13  2025 'transformer nlp 2.ipynb'\n","-rw------- 1 root root  201400 Jan 13  2025 'Transformer nlp.ipynb'\n","-rw------- 1 root root   42200 Jan  4  2025  Transformers.ipynb\n","-rw------- 1 root root  137326 Jan  8  2025 'Uci har dataset final training.ipynb'\n","-rw------- 1 root root  140865 Jan 12  2025 'UCI har dataset final trainning 1.ipynb'\n","-rw------- 1 root root   33571 Jan  4  2025 'uci har dataset.ipynb'\n","-rw------- 1 root root  112031 Jan 19  2025 'UCI HARDATASET PART 3.ipynb'\n","-rw------- 1 root root  107482 Jan  6  2025 'UCIHAR dataset training.ipynb'\n","-rw------- 1 root root   77271 Dec 15  2024 'Unet model.ipynb'\n","-rw------- 1 root root     306 Mar 12  2025  Untitled\n","-rw------- 1 root root    4661 Dec 29  2024  Untitled0.ipynb\n","-rw------- 1 root root   55916 Jan 10  2025  Untitled10.ipynb\n","-rw------- 1 root root   57119 Jan 10  2025  Untitled11.ipynb\n","-rw------- 1 root root 1345605 Jan 12  2025  Untitled12.ipynb\n","-rw------- 1 root root 1263777 Jan 12  2025  Untitled13.ipynb\n","-rw------- 1 root root  161680 Jan 13  2025  Untitled14.ipynb\n","-rw------- 1 root root 1235312 Jan 18  2025  Untitled15.ipynb\n","-rw------- 1 root root   60542 Jan 26  2025  Untitled16.ipynb\n","-rw------- 1 root root    7501 Feb  1  2025  Untitled17.ipynb\n","-rw------- 1 root root   49580 Feb  2  2025  Untitled18.ipynb\n","-rw------- 1 root root   18451 Feb  2  2025  Untitled19.ipynb\n","-rw------- 1 root root    4348 Nov 27  2024  Untitled1.ipynb\n","-rw------- 1 root root  269614 Jul 23 08:35  Untitled20.ipynb\n","-rw------- 1 root root  124611 Jul 23 18:46  Untitled21.ipynb\n","-rw------- 1 root root  102253 Jul 23 19:35  Untitled22.ipynb\n","-rw------- 1 root root   12782 Sep  7 18:44  Untitled23.ipynb\n","-rw------- 1 root root     324 Oct  4 00:28  Untitled24.ipynb\n","-rw------- 1 root root  111226 Jan  4  2025  Untitled2.ipynb\n","-rw------- 1 root root    2891 Jan  4  2025  Untitled3.ipynb\n","-rw------- 1 root root   39226 Jan  7  2025  Untitled4.ipynb\n","-rw------- 1 root root     776 Jan  6  2025  Untitled5.ipynb\n","-rw------- 1 root root   39851 Jan  7  2025  Untitled6.ipynb\n","-rw------- 1 root root   34490 Jan  9  2025  Untitled7.ipynb\n","-rw------- 1 root root   24114 Jan  7  2025  Untitled8.ipynb\n","-rw------- 1 root root  174284 Jan 10  2025  Untitled9.ipynb\n","-rw------- 1 root root   84202 Jan  8  2025 'wisdm dataset training.ipynb'\n","-rw------- 1 root root   38869 Dec 29  2024 'wisdom dataset.ipynb'\n"]}]},{"cell_type":"code","source":["!cp \"/content/drive/My Drive/Colab Notebooks/{NOTEBOOK_NAME}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-hOW203HK2CL","executionInfo":{"status":"ok","timestamp":1763321805722,"user_tz":-360,"elapsed":131,"user":{"displayName":"Nafiz Jaman","userId":"04877804862386982421"}},"outputId":"f6d76fd7-2139-44ee-b09a-38ab4601d453"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["cp: missing destination file operand after '/content/drive/My Drive/Colab Notebooks/mlops-mlflow-experiment.ipynb'\n","Try 'cp --help' for more information.\n"]}]}]}